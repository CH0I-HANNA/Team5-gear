# crawler_server.py
from fastapi import FastAPI, HTTPException
import pymysql
import requests
from bs4 import BeautifulSoup
import re

# -------------------------
# âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
# -------------------------
DB_CONFIG = {
    "host": "localhost",
    "user": "root",
    "password": "hannachoesql53!",
    "database": "gear",
    "cursorclass": pymysql.cursors.DictCursor,
    "charset": "utf8mb4"
}

# -------------------------
# âœ… FastAPI ì•± ìƒì„±
# -------------------------
app = FastAPI(title="GearHub Crawler API", version="2.0.0")

# -------------------------
# âœ… í¬ë¡¤ë§ í•¨ìˆ˜ (ì´ë¯¸ì§€ + ê°€ê²© ì¶”ì¶œ)
# -------------------------
def scrape_product_data(url: str):
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/121.0.0.0 Safari/537.36"
        )
    }

    try:
        res = requests.get(url, headers=headers, timeout=10)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"ìš”ì²­ ì‹¤íŒ¨: {e}")

    if res.status_code != 200:
        raise HTTPException(status_code=400, detail=f"URL ì ‘ê·¼ ì‹¤íŒ¨ (status {res.status_code})")

    soup = BeautifulSoup(res.text, "html.parser")

    # ğŸ” ì´ë¯¸ì§€ URL ì¶”ì¶œ (og:image)
    image_meta = soup.find("meta", property="og:image")
    image_url = image_meta["content"] if image_meta else None

    # ğŸ” ê°€ê²© ì¶”ì¶œ (ì •ê·œì‹ìœ¼ë¡œ "ì›" í¬í•¨ëœ ìˆ«ì íŒ¨í„´ íƒìƒ‰)
    price = None
    price_texts = soup.find_all(string=re.compile(r"\d{1,3}(?:,\d{3})*ì›"))
    if price_texts:
        match = re.search(r"\d{1,3}(?:,\d{3})*", price_texts[0])
        if match:
            price = match.group(0)

    return {"image_url": image_url, "price": price}

# -------------------------
# âœ… DB ì—…ë°ì´íŠ¸ í•¨ìˆ˜
# -------------------------
def update_product_data(equipment_id: int, price: str, image_url: str):
    conn = None
    try:
        conn = pymysql.connect(**DB_CONFIG)
        with conn.cursor() as cursor:
            sql = """
                UPDATE equipment
                SET price = %s,
                    image_url = %s,
                    updated_at = NOW()
                WHERE id = %s
            """
            cursor.execute(sql, (price, image_url, equipment_id))
            conn.commit()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    finally:
        if conn:
            conn.close()

# -------------------------
# âœ… DBì—ì„œ URL ê°€ì ¸ì˜¤ê¸°
# -------------------------
def get_purchase_url(equipment_id: int):
    conn = None
    try:
        conn = pymysql.connect(**DB_CONFIG)
        with conn.cursor() as cursor:
            cursor.execute("SELECT purchase_url FROM equipment WHERE id = %s", (equipment_id,))
            row = cursor.fetchone()
            if not row:
                raise HTTPException(status_code=404, detail="í•´ë‹¹ ì œí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return row["purchase_url"]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
    finally:
        if conn:
            conn.close()

# -------------------------
# âœ… API ì—”ë“œí¬ì¸íŠ¸: ì œí’ˆ ì—…ë°ì´íŠ¸
# -------------------------
@app.post("/update")
def update_existing_product(equipment_id: int):
    # 1) DBì—ì„œ URL ê°€ì ¸ì˜¤ê¸°
    url = get_purchase_url(equipment_id)

    # 2) í•´ë‹¹ URL í¬ë¡¤ë§
    data = scrape_product_data(url)

    if not data["image_url"] and not data["price"]:
        raise HTTPException(status_code=404, detail="ì´ë¯¸ì§€ ë˜ëŠ” ê°€ê²© ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 3) í¬ë¡¤ë§ëœ ë°ì´í„°ë¡œ DB ì—…ë°ì´íŠ¸
    update_product_data(equipment_id, data["price"], data["image_url"])

    return {
        "message": f"{equipment_id}ë²ˆ ì œí’ˆ ì •ë³´ê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "data": data
    }

# -------------------------
# âœ… í—¬ìŠ¤ì²´í¬ (ì„œë²„ í…ŒìŠ¤íŠ¸ìš©)
# -------------------------
@app.get("/health")
def health_check():
    return {"status": "ok", "message": "GearHub Crawler API is running."}

# -------------------------
# âœ… ì‹¤í–‰ ë°©ë²• (í„°ë¯¸ë„)
# -------------------------
# cd /Users/choehanna/Documents/gear
# uvicorn crawler_server:app --reload
